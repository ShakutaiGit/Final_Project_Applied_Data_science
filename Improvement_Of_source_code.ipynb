{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improvement_Of_source_code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPyisgOi98Wwtkcrk3ievdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakutaiGit/Final_Project_Applied_Data_science/blob/main/Improvement_Of_source_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi1KnJZvvsaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22519be6-9cab-4f0f-be1c-dc9eddaec4be"
      },
      "source": [
        "!mkdir outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘outputs’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fc7jmx6wBC4",
        "outputId": "d8874e19-43d8-4b92-b394-b50057cbc9c1"
      },
      "source": [
        "%%writefile models.py\n",
        "from torchvision import models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def resnet34(pretrained, requires_grad):\n",
        "    model = models.resnet34(progress=True, pretrained=pretrained)\n",
        "    # to freeze the hidden layers\n",
        "    if requires_grad == False:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    # to train the hidden layers\n",
        "    elif requires_grad == True:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "    # make the classification layer learnable\n",
        "    # we have 10 classes in total for the CIFAR10 dataset\n",
        "    model.fc = nn.Linear(512, 10)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJynypvFwFNX",
        "outputId": "3c4abd75-9d69-421e-ea4a-69fbbf99fcb4"
      },
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import models\n",
        "import argparse\n",
        "import joblib\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-wr', '--warm-restart', dest='warm_restart', \n",
        "                    action='store_true')\n",
        "parser.add_argument('-t0', '--t-zero', dest='t_zero', type=int,\n",
        "                    default=50)\n",
        "parser.add_argument('-tm', '--t-mult', dest='t_mult', type=int,\n",
        "                    default=1)\n",
        "parser.add_argument('-e', '--epochs', type=int, default=100)\n",
        "args = vars(parser.parse_args())\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"[INFO]: Computation device: {device}\")\n",
        "epochs = args['epochs']\n",
        "batch_size = 128 # same the original paper\n",
        "\n",
        "# we will apply the same transforms as described in the paper\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(size=(32, 32), padding=4, padding_mode='reflect'),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "val_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, \n",
        "                                             transform=train_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, \n",
        "                                           transform=val_transform)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=False)\n",
        "\n",
        "# instantiate the model\n",
        "# we will train all the layers' parameters from scratch\n",
        "model = models.resnet34(pretrained=False, requires_grad=True).to(device)\n",
        "# total parameters and trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"[INFO]: {total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"[INFO]: {total_trainable_params:,} trainable parameters.\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, \n",
        "                      weight_decay=0.0005)\n",
        "\n",
        "# when using warm restarts\n",
        "if args['warm_restart']:\n",
        "    print('[INFO]: Initializing Cosine Annealing with Warm Restart Scheduler')\n",
        "    steps = args['t_zero']\n",
        "    mult = args['t_mult']\n",
        "    print(f\"[INFO]: Number of epochs for first restart: {steps}\")\n",
        "    print(f\"[INFO]: Multiplicative factor: {mult}\")\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, \n",
        "        T_0=steps, \n",
        "        T_mult=mult,\n",
        "        verbose=True\n",
        "    )\n",
        "    loss_plot_name = f\"wr_loss_s{steps}_m{mult}\"\n",
        "    train_loss_list = f\"wr_train_loss_s{steps}_m{mult}\"\n",
        "    val_loss_list = f\"wr_val_loss_s{steps}_m{mult}\"\n",
        "# when not using warm restarts\n",
        "elif args['warm_restart'] == False:\n",
        "    print('[INFO]: Using default Multi Step LR scheduler')\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                                     milestones=[60, 120, 160],\n",
        "                                                     gamma=0.2)\n",
        "    loss_plot_name = 'loss'\n",
        "    train_loss_list = 'train_loss'\n",
        "    val_loss_list = 'val_loss'\n",
        "\n",
        "# training\n",
        "def train(model, trainloader, optimizer, criterion, scheduler, epoch):\n",
        "    model.train()\n",
        "    print('Training')\n",
        "    # we will use this list to store the updated learning rates per epoch\n",
        "    lrs = []\n",
        "    train_running_loss = 0.0\n",
        "    iters = len(trainloader)\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
        "        counter += 1\n",
        "        if args['warm_restart']:\n",
        "            lrs.append(scheduler.get_last_lr()[0])\n",
        "            # print the LR after each 500 iterations\n",
        "            if counter % 500 == 0:\n",
        "                print(f\"[INFO]: LR at iteration {counter}: {scheduler.get_last_lr()}\")\n",
        "        \n",
        "        image, labels = data\n",
        "        image = image.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if using warm restart, then update after each batch iteration\n",
        "        if args['warm_restart']:\n",
        "            scheduler.step(epoch + i / iters)\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "    \n",
        "    epoch_loss = train_running_loss / counter\n",
        "    return lrs, epoch_loss\n",
        "\n",
        "# validation\n",
        "def validate(model, testloader, criterion):\n",
        "    model.eval()\n",
        "    print('Validation')\n",
        "    val_running_loss = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
        "        counter += 1\n",
        "        \n",
        "        image, labels = data\n",
        "        image = image.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_running_loss += loss.item()\n",
        "        \n",
        "    epoch_loss = val_running_loss / counter\n",
        "    return epoch_loss\n",
        "\n",
        "# start the training\n",
        "train_loss, val_loss = [], []\n",
        "learning_rate_plot = []\n",
        "for epoch in range(epochs):\n",
        "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
        "    \n",
        "    print(f\"[INFO]: Current LR [Epoch Begin]: {scheduler.get_last_lr()}\")\n",
        "    lrs, train_epoch_loss = train(model, train_dataloader, optimizer, \n",
        "                                  criterion, scheduler, epoch)\n",
        "    val_epoch_loss = validate(model, val_dataloader, criterion)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    learning_rate_plot.extend(lrs)\n",
        "\n",
        "    # if not using warm restart, then check whether to update MultiStepLR\n",
        "    if args['warm_restart'] == False:\n",
        "        scheduler.step() # take default MultiStepLR\n",
        "    print(f\"[INFO]: Current LR [Epoch end]: {scheduler.get_last_lr()}\")\n",
        "    print(f\"Training loss: {train_epoch_loss:.3f}\")\n",
        "    print(f\"Validation loss: {val_epoch_loss:.3f}\")\n",
        "    print('------------------------------------------------------------')\n",
        "print('Finished Training')\n",
        "\n",
        "if args['warm_restart']:\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(learning_rate_plot, color='blue', label='lr')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('lr')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"outputs/lr_schedule_s{steps}_m{mult}.jpg\")\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(f\"outputs/{loss_plot_name}.jpg\")\n",
        "\n",
        "# serialize the loss lists to disk\n",
        "if args['warm_restart']:\n",
        "    joblib.dump(train_loss, f\"outputs/{train_loss_list}.pkl\")\n",
        "    joblib.dump(val_loss, f\"outputs/{val_loss_list}.pkl\")\n",
        "else:\n",
        "    joblib.dump(train_loss, f\"outputs/{train_loss_list}.pkl\")\n",
        "    joblib.dump(val_loss, f\"outputs/{val_loss_list}.pkl\")\n",
        "\n",
        "print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGUL-PbewFkO",
        "outputId": "4b67f00b-c9c0-41c9-f62a-9ea8af8cd51b"
      },
      "source": [
        "%%writefile run.sh\n",
        "python train.py -e 200\n",
        "\n",
        "python train.py -e 200 --warm-restart -t0 50\n",
        "\n",
        "python train.py -e 200 --warm-restart -t0 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing run.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_7SU_btwHSA",
        "outputId": "a522537e-65b0-440a-8cf2-c13bcda6c38e"
      },
      "source": [
        "!sh run.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Computation device: cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "170499072it [00:11, 14920308.48it/s]                   \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[INFO]: 21,289,802 total parameters.\n",
            "[INFO]: 21,289,802 trainable parameters.\n",
            "[INFO]: Using default Multi Step LR scheduler\n",
            "[INFO]: Epoch 1 of 200\n",
            "[INFO]: Current LR [Epoch Begin]: [0.05]\n",
            "Training\n",
            "  0% 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "  6% 25/391 [01:09<16:40,  2.73s/it]Traceback (most recent call last):\n",
            "  File \"train.py\", line 160, in <module>\n",
            "    criterion, scheduler, epoch)\n",
            "  File \"train.py\", line 118, in train\n",
            "    outputs = model(image)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\", line 249, in forward\n",
            "    return self._forward_impl(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\", line 240, in _forward_impl\n",
            "    x = self.layer4(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\", line 70, in forward\n",
            "    out = self.conv1(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 440, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "KeyboardInterrupt\n",
            "  6% 25/391 [01:10<17:07,  2.81s/it]\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "UR6qiJdzwJcZ",
        "outputId": "19f64316-3127-4647-fef7-1d907e039fca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3ccf3d4d43e5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    outputs\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}