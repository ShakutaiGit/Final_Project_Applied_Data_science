{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNetB3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakutaiGit/Final_Project_Applied_Data_science/blob/main/Applied_data_science_Flower_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDS1pcLMLiLj"
      },
      "source": [
        "Import the relevant libraries and define global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTTfy83J9Iej"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model \n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
        "\n",
        "input_size = (300,300,3)\n",
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "size=300\n",
        "# so we can find the best weights in the model.\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint('best_weights.hdf5', save_best_only=True, save_weights_only=True, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz8xwueAehz7"
      },
      "source": [
        "\n",
        "Loading the data and importing the relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Ap50LJeewX"
      },
      "source": [
        "train_ds ,train_info= tfds.load('oxford_flowers102', split='train[:80%]', with_info=True)\n",
        "val_ds,val_info = tfds.load('oxford_flowers102', split='train[80%:90%]', with_info=True)\n",
        "test_ds , test_info = tfds.load('oxford_flowers102', split='train[90%:]', with_info=True)\n",
        "# two different  splits \n",
        "train_ds1 ,train_info1= tfds.load('oxford_flowers102', split='train[20%:]', with_info=True)\n",
        "val_ds1,val_info1 = tfds.load('oxford_flowers102', split='train[10%:20%]', with_info=True)\n",
        "test_ds1 , test_info1 = tfds.load('oxford_flowers102', split='train[:10%]', with_info=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShPNQ7hPwfOe"
      },
      "source": [
        "Pre Processing of the data- agumentation , resize , normlization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTvOY8QR8YH3"
      },
      "source": [
        "# pre processing the data \n",
        "\n",
        "# augmentation function \n",
        "data_augmentation = tf.keras.Sequential([\n",
        "\n",
        "            layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "            layers.experimental.preprocessing.RandomZoom(0.1)\n",
        "            #layers.experimental.preprocessing.RandomContrast(factor=0.2)\n",
        "])\n",
        "# normlizing and resizing function \n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "layers.experimental.preprocessing.Resizing(size, size)\n",
        "])\n",
        "\n",
        "def prepare(ds):\n",
        "  # Resize and rescale all kind datasets\n",
        "  ds = ds.map(lambda data: (resize_and_rescale(data['image']), tf.one_hot(data['label'], 102)), \n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "  # Batch all datasets\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrv09dpwsHH"
      },
      "source": [
        "ploting the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MYkhsWyuyyN"
      },
      "source": [
        "def show_history(history):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "    ax[0].set_title('loss')\n",
        "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "    ax[1].set_title('acc')\n",
        "    ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n",
        "    ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n",
        "    ax[0].legend()\n",
        "    ax[1].legend()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brCbNJc8T9DN"
      },
      "source": [
        " The typical transfer-learning workflow\n",
        " This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
        "\n",
        " Instantiate a base model and load pre-trained weights into it.\n",
        " Freeze all layers in the base model by setting trainable = False.\n",
        " Create a new model on top of the output of one (or several) layers from the base model.\n",
        " Train your new model on your new dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHrvN4oHwwss"
      },
      "source": [
        "EfficientNetB3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoS5c_e-_ofD"
      },
      "source": [
        "\n",
        "def eff_net_model(train_ds,val_ds,test_ds):\n",
        "  print(\"-------------EfficientNetB3  model--------------\")\n",
        "  train_ds= prepare(train_ds)\n",
        "  val_ds= prepare(val_ds)\n",
        "  test_ds= prepare(test_ds)\n",
        "\n",
        "  inputs = layers.Input(shape=(300, 300, 3))\n",
        "  x = data_augmentation(inputs)  \n",
        "  pre_trained_model = keras.applications.EfficientNetB3(include_top=False, weights='imagenet',input_tensor=x,pooling='avg')\n",
        "  pre_trained_model.trainable= False\n",
        "  x = layers.BatchNormalization()(pre_trained_model.output)\n",
        "  top_dropout_rate = 0.5\n",
        "  x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "  outputs = layers.Dense(102, activation=\"softmax\", name=\"pred\")(x)\n",
        "  model = tf.keras.Model(inputs, outputs, name=\"ResNet\")\n",
        "\n",
        "  #unfreeze some layers of the pretrained model\n",
        "  for layer in model.layers[-15:]:\n",
        "    if not isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = True\n",
        "\n",
        "  optimizer ='adam'\n",
        "  model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"]\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  model_info = model.fit(train_ds, epochs=40, callbacks=[cp_callback], validation_data=val_ds)\n",
        "\n",
        "  print(\"  Training Accuracy: {}% , Training Loss {}\".format(max(model_info.history['acc'])*100,min(model_info.history['loss'])))\n",
        "  print(\" Validation accuracy: {}% , Validation Loss {}\".format(max(model_info.history['val_acc'])*100, min(model_info.history['val_loss'])))\n",
        "  loss, test = model.evaluate(test_ds, verbose=0)\n",
        "  print(\"Test accuracy: {}% , Test  loss : {}\".format(test * 100,loss))\n",
        "  return model_info "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjoZmclw0MO"
      },
      "source": [
        "ResNet50 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cy-S2_MN9il"
      },
      "source": [
        "\n",
        "def res_net_50_model(train_ds,val_ds,test_ds):\n",
        "  print(\"-------------ResNet50 model--------------\")\n",
        "  train_ds= prepare(train_ds)\n",
        "  val_ds= prepare(val_ds)\n",
        "  test_ds= prepare(test_ds)\n",
        "\n",
        "\n",
        "  inputs = layers.Input(shape=(300, 300, 3))\n",
        "  x = data_augmentation(inputs)  \n",
        "  pre_trained_model = keras.applications.ResNet50(include_top=False, weights='imagenet',input_tensor=x,pooling='avg')\n",
        "  pre_trained_model.trainable= False\n",
        "  #x = layers.GlobalAveragePooling3D(name=\"avg_pool\")(pre_trained_model.output)\n",
        "  x = layers.BatchNormalization()(pre_trained_model.output)\n",
        "  top_dropout_rate = 0.5\n",
        "  x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "  outputs = layers.Dense(102, activation=\"softmax\", name=\"pred\")(x)\n",
        "  model = tf.keras.Model(inputs, outputs, name=\"ResNet\")\n",
        "  #unfreeze some layers of the pretrained model\n",
        "  for layer in model.layers[-10:]:\n",
        "    if not isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = True\n",
        "\n",
        "  optimizer ='adam'\n",
        "  model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"]\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  model_info = model.fit(train_ds, epochs=30, callbacks=[cp_callback], validation_data=val_ds)\n",
        "\n",
        "  print(\"  Training Accuracy: {}% , Training Loss {}\".format(max(model_info.history['acc'])*100,min(model_info.history['loss'])))\n",
        "  print(\" Validation accuracy: {}% , Validation Loss {}\".format(max(model_info.history['val_acc'])*100, min(model_info.history['val_loss'])))\n",
        "  loss, test = model.evaluate(test_ds, verbose=0)\n",
        "  print(\"Test accuracy: {}% , Test  loss : {}\".format(test * 100,loss))\n",
        "  return model_info "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UR3dKRlNEkx"
      },
      "source": [
        "show_eff = eff_net_model(train_ds,val_ds,test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7V4W3SwNEPk"
      },
      "source": [
        "show_eff1 = eff_net_model(train_ds1,val_ds1,test_ds1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NBp5Z_FND7_"
      },
      "source": [
        "show_res = res_net_50_model(train_ds,val_ds,test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFhnGaONDry"
      },
      "source": [
        "show_res1= res_net_50_model(train_ds1,val_ds1,test_ds1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjBz13zuPmHz"
      },
      "source": [
        "show_history(show_eff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_uXuXptPl2O"
      },
      "source": [
        "show_history(show_eff1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_aSZTBsPlsy"
      },
      "source": [
        "show_history(show_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZkpCSyPlfE"
      },
      "source": [
        "show_history(show_res1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
